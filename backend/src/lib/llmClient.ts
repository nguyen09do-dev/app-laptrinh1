import OpenAI from 'openai';
import { GoogleGenerativeAI } from '@google/generative-ai';

/**
 * Enum cho c√°c AI provider
 */
export enum AIProvider {
  OPENAI = 'openai',
  GEMINI = 'gemini',
}

/**
 * Enum cho c√°c model c√≥ s·∫µn
 */
export enum AIModel {
  // OpenAI models
  GPT_4 = 'gpt-4',
  GPT_4_TURBO = 'gpt-4-turbo-preview',
  GPT_35_TURBO = 'gpt-3.5-turbo',

  // Gemini models (latest versions)
  GEMINI_PRO_LATEST = 'gemini-pro-latest',
  GEMINI_FLASH_LATEST = 'gemini-flash-latest',

  // Gemini 2.5 models (stable)
  GEMINI_25_PRO = 'gemini-2.5-pro',
  GEMINI_25_FLASH = 'gemini-2.5-flash',

  // Gemini 2.0 models
  GEMINI_20_FLASH = 'gemini-2.0-flash',
}

/**
 * LLMClient - Class ƒë·ªÉ giao ti·∫øp v·ªõi AI providers (OpenAI, Gemini)
 * H·ªó tr·ª£ chuy·ªÉn ƒë·ªïi linh ho·∫°t gi·ªØa c√°c providers
 */
export class LLMClient {
  private openaiClient: OpenAI;
  private geminiClient: GoogleGenerativeAI;
  private defaultProvider: AIProvider;

  constructor() {
    // Kh·ªüi t·∫°o OpenAI client
    this.openaiClient = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY || '',
    });

    // Kh·ªüi t·∫°o Gemini client
    this.geminiClient = new GoogleGenerativeAI(
      process.env.GEMINI_API_KEY || ''
    );

    // Set default provider t·ª´ env ho·∫∑c m·∫∑c ƒë·ªãnh l√† OpenAI
    const envProvider = process.env.DEFAULT_AI_PROVIDER?.toLowerCase();
    this.defaultProvider = envProvider === 'gemini'
      ? AIProvider.GEMINI
      : AIProvider.OPENAI;
  }

  /**
   * G·ªçi OpenAI API ƒë·ªÉ generate text
   */
  private async generateWithOpenAI(
    prompt: string,
    model: string,
    temperature: number
  ): Promise<string> {
    const response = await this.openaiClient.chat.completions.create({
      model,
      messages: [
        {
          role: 'system',
          content: 'You are a helpful content strategist. Always respond with valid JSON only, no markdown formatting.',
        },
        {
          role: 'user',
          content: prompt,
        },
      ],
      temperature,
      max_tokens: 2000,
    });

    const content = response.choices[0]?.message?.content;

    if (!content) {
      throw new Error('No content in OpenAI response');
    }

    return content;
  }

  /**
   * G·ªçi Gemini API ƒë·ªÉ generate text
   */
  private async generateWithGemini(
    prompt: string,
    model: string,
    temperature: number
  ): Promise<string> {
    const geminiModel = this.geminiClient.getGenerativeModel({
      model,
      generationConfig: {
        temperature,
        maxOutputTokens: 8000, // Increase for longer content
      },
    });

    const result = await geminiModel.generateContent(prompt);
    const response = result.response;

    // Check if response was blocked
    if (!response || !response.text) {
      const blockReason = response?.promptFeedback?.blockReason;
      if (blockReason) {
        throw new Error(`Gemini blocked response: ${blockReason}`);
      }
      throw new Error('No content in Gemini response');
    }

    const content = response.text();

    if (!content || content.trim().length === 0) {
      throw new Error('Empty content in Gemini response');
    }

    return content;
  }

  /**
   * G·ªçi AI API ƒë·ªÉ generate text - H·ªó tr·ª£ c·∫£ OpenAI v√† Gemini
   * @param prompt - C√¢u l·ªánh ƒë·∫ßu v√†o
   * @param modelOrOptions - Model string (legacy) ho·∫∑c options object
   * @param temperature - Temperature (ch·ªâ d√πng v·ªõi legacy call)
   * @returns N·ªôi dung response t·ª´ AI
   */
  async generateCompletion(
    prompt: string,
    modelOrOptions?: string | {
      provider?: AIProvider;
      model?: AIModel | string;
      temperature?: number;
    },
    temperature?: number
  ): Promise<string> {
    // X·ª≠ l√Ω backward compatibility
    let provider: AIProvider;
    let model: string;
    let temp: number;

    if (typeof modelOrOptions === 'string') {
      // Legacy call: generateCompletion(prompt, model, temperature)
      provider = this.defaultProvider;
      model = modelOrOptions;
      temp = temperature ?? 0.7;
    } else {
      // New call: generateCompletion(prompt, { provider, model, temperature })
      provider = modelOrOptions?.provider || this.defaultProvider;
      temp = modelOrOptions?.temperature ?? 0.7;

      if (modelOrOptions?.model) {
        model = modelOrOptions.model;
      } else {
        model = provider === AIProvider.GEMINI
          ? AIModel.GEMINI_25_FLASH
          : AIModel.GPT_35_TURBO;
      }
    }

    try {
      console.log(`ü§ñ Calling ${provider.toUpperCase()} with model: ${model}`);

      if (provider === AIProvider.OPENAI) {
        return await this.generateWithOpenAI(prompt, model, temp);
      } else {
        return await this.generateWithGemini(prompt, model, temp);
      }
    } catch (error) {
      console.error(`‚ùå ${provider.toUpperCase()} API Error:`, error);
      throw error;
    }
  }

  /**
   * L·∫•y danh s√°ch models c√≥ s·∫µn theo provider
   */
  getAvailableModels(provider: AIProvider): string[] {
    if (provider === AIProvider.OPENAI) {
      return [
        AIModel.GPT_4,
        AIModel.GPT_4_TURBO,
        AIModel.GPT_35_TURBO,
      ];
    } else {
      return [
        AIModel.GEMINI_PRO_LATEST,
        AIModel.GEMINI_FLASH_LATEST,
        AIModel.GEMINI_25_PRO,
        AIModel.GEMINI_25_FLASH,
        AIModel.GEMINI_20_FLASH,
      ];
    }
  }

  /**
   * L·∫•y t·∫•t c·∫£ models t·ª´ t·∫•t c·∫£ providers
   */
  getAllAvailableModels(): { provider: AIProvider; models: string[] }[] {
    return [
      { provider: AIProvider.OPENAI, models: this.getAvailableModels(AIProvider.OPENAI) },
      { provider: AIProvider.GEMINI, models: this.getAvailableModels(AIProvider.GEMINI) },
    ];
  }

  /**
   * L·∫•y default provider hi·ªán t·∫°i
   */
  getDefaultProvider(): AIProvider {
    return this.defaultProvider;
  }

  /**
   * Thay ƒë·ªïi default provider
   */
  setDefaultProvider(provider: AIProvider): void {
    this.defaultProvider = provider;
  }
}

// Export singleton instance
export const llmClient = new LLMClient();







